---
title: "About Me"
date: 2024-06-08T00:00:00Z
draft: false
---

Hi there, I'm Lei Zhang. ðŸ‘‹

I'm a Ph.D. student at the **University of Chinese Academy of Sciences**, Beijing, China.
My research interests include **Agentic Coding**, **Agentic Reinforcement Learning**, and **Natural Language Processing**.

I'm currently interning at **Alibaba Qwen Group** in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.

## Academic Achievements

- **Ph.D. in Computer Application Technology**, University of Chinese Academy of Sciences, 2021-present
- **B.Sc. in Computer Science**, University of Chinese Academy of Sciences, 2017-2021

## Technical Reports
- Qwen3-Coder Technical Report, **[Technical Report (Under Preparation)]**

    *Core Contributor* â€” Responsible for Agentic Coding (including infrastructure implementation and training data synthesis)

- Qwen3 Technical Report, **[Technical Report]**

    *Core Contributor* â€” Responsible for Agentic Coding (including infrastructure implementation and training data synthesis)

- Qwen2.5-Coder Technical Report, **[Technical Report]**

    *Binyuan Hui*, *Jian Yang*, *Zeyu Cui*, *Jiaxi Yang*, *Dayiheng Liu*, ***Lei Zhang***, *Tianyu Liu*, *Jiajun Zhang*, *Bowen Yu*, *Kai Dang*, *An Yang*, *Rui Men*, *Fei Huang*, *Xingzhang Ren*, *Xuancheng Ren*, *Jingren Zhou*, *Junyang Lin*

- Qwen2.5 Technical Report, **[Technical Report]**

    *Contributor* â€” Responsible for integrating and conducting code evaluation (EvalPlus, Multiple, LiveCodeBench, BigCodeBench) for iterative model development.


## Publications
- CodeArena: Evaluating and Aligning CodeLLMs on Human Preference, **[EMNLP'25, CCF-B]**

    *Jian Yang*, *Jiaxi Yang*, *Wei Zhang*, *Ke Jin*, *Yibo Miao*, ***Lei Zhang***, *Liqun Yang*, *Zeyu Cui*, *Yichang Zhang*, *Binyuan Hui*, *Junyang Lin*

- SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner, **[ICML'25, CCF-A]** 

    ***Lei Zhang***, *Jiaxi Yang*, *Min Yang*, *Jian Yang*, *Mouxiang Chen*, *Jiajun Zhang*, *Zeyu Cui*, *Binyuan Hui*, *Junyang Lin*

- DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception, **[ICLR'25, Spotlight]**

    *Run Luo*, *Yunshui Li*, *Longze Chen*, *Wanwei He*, *Ting-En Lin*, *Ziqiang Liu*, ***Lei Zhang***, *Zikai Song*, *Xiaobo Xia*, *Tongliang Liu*, *Min Yang*, *Binyuan Hui*

- Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs, **[AAAI'25, CCF-A]**

    ***Lei Zhang***, *Yunshui Li*, *Jiaming Li*, *Xiaobo Xia*, *Jiaxi Yang*, *Run Luo*, *Minzheng Wang*, *Longze Chen*, *Junhao Liu*, *Min Yang*

- Fine-Tuning Language Models with Collaborative and Semantic Experts, **[AAAI'25, CCF-A]**

    *Jiaxi Yang*, *Binyuan Hui*, *Min Yang*, *Jian Yang*, ***Lei Zhang***, *Junyang Lin*, *Chang Zhou*

- Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models, **[EMNLP'24, CCF-B]**

    *Jiaming Li*, ***Lei Zhang***, *Yunshui Li*, *Ziqiang Liu*, *yuelin bai*, *Run Luo*, *Longze Chen*, *Min Yang*

- Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA, **[EMNLP'24, CCF-B]**

    *Minzheng Wang*, *Longze Chen*, *Cheng Fu*, *Liaoshengyi*, *Xinghua Zhang*, *Bingliwu*, *Haiyang Yu*, *Nan Xu*, ***Lei Zhang***, *Run Luo*, *Yunshui Li*, *Min Yang*, *Yongbin Li*

- Marathon: A Race Through the Realm of Long Context with Large Language Models, **[ACL'24, CCF-A]**

    ***Lei Zhang***, *Yunshui Li*, *Ziqiang Liu*, *Jiaxi Yang*, *Junhao Liu*, *Longze Chen*, *Run Luo*, *Min Yang*

- One Shot Learning as Instruction Data Prospector for Large Language Models, **[ACL'24, CCF-A]**

    *Yunshui Li*, *Binyuan Hui*, *Xiaobo Xia*, *Jiaxi Yang*, *Min Yang*, ***Lei Zhang***, *Shuzheng Si*, *Junhao Liu*, *Tongliang Liu*, *Fei Huang*, *Yongbin Li*

- Image-text retrieval via contrastive learning with auxiliary generative features and support-set regularization, **[SIGIR'22, CCF-A]**

    ***Lei Zhang***, *Min Yang*, *Chengming Li*, *Ruifeng Xu*

- ExecRepoBench: Multi-level Executable Code Completion Evaluation, **[Preprint]**

    *Jian Yang*, *Jiaxi Yang*, *Ke Jin*, *Yibo Miao*, ***Lei Zhang***, *Liqun Yang*, *Zeyu Cui*, *Yichang Zhang*, *Binyuan Hui*, *Junyang Lin*

- Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey, **[Preprint]**

    *Liang Chen*, *Zekun Wang*, *Shuhuai Ren*, *Lei Li*, *Haozhe Zhao*, *Yunshui Li*, *Zefan Cai*, *Hongcheng Guo*, ***Lei Zhang***, *Yizhe Xiong*, *Yichi Zhang*, *Ruoyu Wu*, *Qingxiu Dong*, *Ge Zhang*, *Jian Yang*, *Lingwei Meng*, *Shujie Hu*, *Yulong Chen*, *Junyang Lin*, *Shuai Bai*, *Andreas Vlachos*, *Xu Tan*, *Minjia Zhang*, *Wen Xiao*, *Aaron Yee*, *Tianyu Liu*, *Baobao Chang*

- Openomni: Large language models pivot zero-shot omnimodal alignment across language with real-time self-aware emotional speech synthesis, **[Preprint]**

    *Run Luo*, *Ting-En Lin*, *Haonan Zhang*, *Yuchuan Wu*, *Xiong Liu*, *Min Yang*, *Yongbin Li*, *Longze Chen*, *Jiaming Li*, ***Lei Zhang***, *Xiaobo Xia*, *Hamid Alinejad-Rokny*, *Fei Huang*

## Research Interests

- **Agentic Coding**
- **Agentic Reinforcement Learning**
- **Natural Language Processing**

## Contact

Feel free to reach out via [lei.zhang2@siat.ac.cn](mailto:lei.zhang2@siat.ac.cn) for collaboration or questions. 

<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>About Me | Homepage</title><meta name=keywords content><meta name=description content="Hi there, I&rsquo;m Lei Zhang. ðŸ‘‹
I&rsquo;m a Ph.D. student at the University of Chinese Academy of Sciences, Beijing, China.
My research interests include Agentic Coding, Agentic Reinforcement Learning, and Natural Language Processing.
Iâ€™m currently interning at Alibaba Qwen Group in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.
Academic Achievements

Ph.D. in Computer Science, University of Chinese Academy of Sciences, 2021-present
B.Sc. in Computer Science, University of Chinese Academy of Sciences, 2017-2021

Publications


Qwen3 Technical Report, [Technical Report]"><meta name=author content="Lei Zhang"><link rel=canonical href=http://hambaobao.me/about/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://hambaobao.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://hambaobao.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://hambaobao.me/favicon-32x32.png><link rel=apple-touch-icon href=http://hambaobao.me/apple-touch-icon.png><link rel=mask-icon href=http://hambaobao.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://hambaobao.me/about/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/css/override.css><meta property="og:url" content="http://hambaobao.me/about/"><meta property="og:site_name" content="Homepage"><meta property="og:title" content="About Me"><meta property="og:description" content="Hi there, Iâ€™m Lei Zhang. ðŸ‘‹
Iâ€™m a Ph.D. student at the University of Chinese Academy of Sciences, Beijing, China. My research interests include Agentic Coding, Agentic Reinforcement Learning, and Natural Language Processing.
Iâ€™m currently interning at Alibaba Qwen Group in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.
Academic Achievements Ph.D. in Computer Science, University of Chinese Academy of Sciences, 2021-present B.Sc. in Computer Science, University of Chinese Academy of Sciences, 2017-2021 Publications Qwen3 Technical Report, [Technical Report]"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:published_time" content="2024-06-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="About Me"><meta name=twitter:description content="Hi there, I&rsquo;m Lei Zhang. ðŸ‘‹
I&rsquo;m a Ph.D. student at the University of Chinese Academy of Sciences, Beijing, China.
My research interests include Agentic Coding, Agentic Reinforcement Learning, and Natural Language Processing.
Iâ€™m currently interning at Alibaba Qwen Group in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.
Academic Achievements

Ph.D. in Computer Science, University of Chinese Academy of Sciences, 2021-present
B.Sc. in Computer Science, University of Chinese Academy of Sciences, 2017-2021

Publications


Qwen3 Technical Report, [Technical Report]"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"About Me","item":"http://hambaobao.me/about/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"About Me","name":"About Me","description":"Hi there, I\u0026rsquo;m Lei Zhang. ðŸ‘‹\nI\u0026rsquo;m a Ph.D. student at the University of Chinese Academy of Sciences, Beijing, China. My research interests include Agentic Coding, Agentic Reinforcement Learning, and Natural Language Processing.\nIâ€™m currently interning at Alibaba Qwen Group in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.\nAcademic Achievements Ph.D. in Computer Science, University of Chinese Academy of Sciences, 2021-present B.Sc. in Computer Science, University of Chinese Academy of Sciences, 2017-2021 Publications Qwen3 Technical Report, [Technical Report]\n","keywords":[],"articleBody":"Hi there, Iâ€™m Lei Zhang. ðŸ‘‹\nIâ€™m a Ph.D. student at the University of Chinese Academy of Sciences, Beijing, China. My research interests include Agentic Coding, Agentic Reinforcement Learning, and Natural Language Processing.\nIâ€™m currently interning at Alibaba Qwen Group in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.\nAcademic Achievements Ph.D. in Computer Science, University of Chinese Academy of Sciences, 2021-present B.Sc. in Computer Science, University of Chinese Academy of Sciences, 2017-2021 Publications Qwen3 Technical Report, [Technical Report]\nContributors\nSWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner, [ICML'25, CCF-A]\nLei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, Junyang Lin\nDEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception, [ICLR'25]\nRun Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui\nHierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs, [AAAI'25, CCF-A]\nLei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang\nFine-Tuning Language Models with Collaborative and Semantic Experts, [AAAI'25, CCF-A]\nJiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Lei Zhang, Junyang Lin, Chang Zhou\nExecRepoBench: Multi-level Executable Code Completion Evaluation, [Preprint]\nJian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, Junyang Lin\nEvaluating and Aligning CodeLLMs on Human Preference, [Preprint]\nJian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, Junyang Lin\nNext Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey, [Preprint]\nLiang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang\nQwen2.5 Technical Report, [Technical Report]\nContributors\nQwen2.5-Coder Technical Report, [Technical Report]\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin\nRuler: A Model-Agnostic Method to Control Generated Length for Large Language Models, [EMNLP'24, CCF-B]\nJiaming Li, Lei Zhang, Yunshui Li, Ziqiang Liu, yuelin bai, Run Luo, Longze Chen, Min Yang\nLeave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA, [EMNLP'24, CCF-B]\nMinzheng Wang, Longze Chen, Cheng Fu, Liaoshengyi, Xinghua Zhang, Bingliwu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Yongbin Li\nMarathon: A Race Through the Realm of Long Context with Large Language Models, [ACL'24, CCF-A]\nLei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi Yang, Junhao Liu, Longze Chen, Run Luo, Min Yang\nOne Shot Learning as Instruction Data Prospector for Large Language Models, [ACL'24, CCF-A]\nYunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li\nImage-text retrieval via contrastive learning with auxiliary generative features and support-set regularization, [SIGIR'22, CCF-A]\nLei Zhang, Min Yang, Chengming Li, Ruifeng Xu\nResearch Interests Agentic Coding Agentic Reinforcement Learning Natural Language Processing Contact Feel free to reach out via lei.zhang2@siat.ac.cn for collaboration or questions.\n","wordCount":"551","inLanguage":"en","datePublished":"2024-06-08T00:00:00Z","dateModified":"2024-06-08T00:00:00Z","author":{"@type":"Person","name":"Lei Zhang"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://hambaobao.me/about/"},"publisher":{"@type":"Organization","name":"Homepage","logo":{"@type":"ImageObject","url":"http://hambaobao.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://hambaobao.me/ accesskey=h title="Homepage (Alt + H)">Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://hambaobao.me/about/ title=About><span class=active>About</span></a></li><li><a href=http://hambaobao.me/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">About Me</h1><div class=post-meta><span title='2024-06-08 00:00:00 +0000 UTC'>June 8, 2024</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;Lei Zhang</div></header><div class=post-content><p>Hi there, I&rsquo;m Lei Zhang. ðŸ‘‹</p><p>I&rsquo;m a Ph.D. student at the <strong>University of Chinese Academy of Sciences</strong>, Beijing, China.
My research interests include <strong>Agentic Coding</strong>, <strong>Agentic Reinforcement Learning</strong>, and <strong>Natural Language Processing</strong>.</p><p>Iâ€™m currently interning at <strong>Alibaba Qwen Group</strong> in Beijing, China, conducting research on Large Language Models (LLMs) and their applications.</p><h2 id=academic-achievements>Academic Achievements<a hidden class=anchor aria-hidden=true href=#academic-achievements>#</a></h2><ul><li><strong>Ph.D. in Computer Science</strong>, University of Chinese Academy of Sciences, 2021-present</li><li><strong>B.Sc. in Computer Science</strong>, University of Chinese Academy of Sciences, 2017-2021</li></ul><h2 id=publications>Publications<a hidden class=anchor aria-hidden=true href=#publications>#</a></h2><ul><li><p>Qwen3 Technical Report, <strong>[Technical Report]</strong></p><p><em>Contributors</em></p></li><li><p>SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner, <strong>[ICML'25, CCF-A]</strong></p><p><em><strong>Lei Zhang</strong></em>, <em>Jiaxi Yang</em>, <em>Min Yang</em>, <em>Jian Yang</em>, <em>Mouxiang Chen</em>, <em>Jiajun Zhang</em>, <em>Zeyu Cui</em>, <em>Binyuan Hui</em>, <em>Junyang Lin</em></p></li><li><p>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception, <strong>[ICLR'25]</strong></p><p><em>Run Luo</em>, <em>Yunshui Li</em>, <em>Longze Chen</em>, <em>Wanwei He</em>, <em>Ting-En Lin</em>, <em>Ziqiang Liu</em>, <em><strong>Lei Zhang</strong></em>, <em>Zikai Song</em>, <em>Xiaobo Xia</em>, <em>Tongliang Liu</em>, <em>Min Yang</em>, <em>Binyuan Hui</em></p></li><li><p>Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs, <strong>[AAAI'25, CCF-A]</strong></p><p><em><strong>Lei Zhang</strong></em>, <em>Yunshui Li</em>, <em>Jiaming Li</em>, <em>Xiaobo Xia</em>, <em>Jiaxi Yang</em>, <em>Run Luo</em>, <em>Minzheng Wang</em>, <em>Longze Chen</em>, <em>Junhao Liu</em>, <em>Min Yang</em></p></li><li><p>Fine-Tuning Language Models with Collaborative and Semantic Experts, <strong>[AAAI'25, CCF-A]</strong></p><p><em>Jiaxi Yang</em>, <em>Binyuan Hui</em>, <em>Min Yang</em>, <em>Jian Yang</em>, <em><strong>Lei Zhang</strong></em>, <em>Junyang Lin</em>, <em>Chang Zhou</em></p></li><li><p>ExecRepoBench: Multi-level Executable Code Completion Evaluation, <strong>[Preprint]</strong></p><p><em>Jian Yang</em>, <em>Jiaxi Yang</em>, <em>Ke Jin</em>, <em>Yibo Miao</em>, <em><strong>Lei Zhang</strong></em>, <em>Liqun Yang</em>, <em>Zeyu Cui</em>, <em>Yichang Zhang</em>, <em>Binyuan Hui</em>, <em>Junyang Lin</em></p></li><li><p>Evaluating and Aligning CodeLLMs on Human Preference, <strong>[Preprint]</strong></p><p><em>Jian Yang</em>, <em>Jiaxi Yang</em>, <em>Ke Jin</em>, <em>Yibo Miao</em>, <em><strong>Lei Zhang</strong></em>, <em>Liqun Yang</em>, <em>Zeyu Cui</em>, <em>Yichang Zhang</em>, <em>Binyuan Hui</em>, <em>Junyang Lin</em></p></li><li><p>Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey, <strong>[Preprint]</strong></p><p><em>Liang Chen</em>, <em>Zekun Wang</em>, <em>Shuhuai Ren</em>, <em>Lei Li</em>, <em>Haozhe Zhao</em>, <em>Yunshui Li</em>, <em>Zefan Cai</em>, <em>Hongcheng Guo</em>, <em><strong>Lei Zhang</strong></em>, <em>Yizhe Xiong</em>, <em>Yichi Zhang</em>, <em>Ruoyu Wu</em>, <em>Qingxiu Dong</em>, <em>Ge Zhang</em>, <em>Jian Yang</em>, <em>Lingwei Meng</em>, <em>Shujie Hu</em>, <em>Yulong Chen</em>, <em>Junyang Lin</em>, <em>Shuai Bai</em>, <em>Andreas Vlachos</em>, <em>Xu Tan</em>, <em>Minjia Zhang</em>, <em>Wen Xiao</em>, <em>Aaron Yee</em>, <em>Tianyu Liu</em>, <em>Baobao Chang</em></p></li><li><p>Qwen2.5 Technical Report, <strong>[Technical Report]</strong></p><p><em>Contributors</em></p></li><li><p>Qwen2.5-Coder Technical Report, <strong>[Technical Report]</strong></p><p><em>Binyuan Hui</em>, <em>Jian Yang</em>, <em>Zeyu Cui</em>, <em>Jiaxi Yang</em>, <em>Dayiheng Liu</em>, <em><strong>Lei Zhang</strong></em>, <em>Tianyu Liu</em>, <em>Jiajun Zhang</em>, <em>Bowen Yu</em>, <em>Kai Dang</em>, <em>An Yang</em>, <em>Rui Men</em>, <em>Fei Huang</em>, <em>Xingzhang Ren</em>, <em>Xuancheng Ren</em>, <em>Jingren Zhou</em>, <em>Junyang Lin</em></p></li><li><p>Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models, <strong>[EMNLP'24, CCF-B]</strong></p><p><em>Jiaming Li</em>, <em><strong>Lei Zhang</strong></em>, <em>Yunshui Li</em>, <em>Ziqiang Liu</em>, <em>yuelin bai</em>, <em>Run Luo</em>, <em>Longze Chen</em>, <em>Min Yang</em></p></li><li><p>Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA, <strong>[EMNLP'24, CCF-B]</strong></p><p><em>Minzheng Wang</em>, <em>Longze Chen</em>, <em>Cheng Fu</em>, <em>Liaoshengyi</em>, <em>Xinghua Zhang</em>, <em>Bingliwu</em>, <em>Haiyang Yu</em>, <em>Nan Xu</em>, <em><strong>Lei Zhang</strong></em>, <em>Run Luo</em>, <em>Yunshui Li</em>, <em>Min Yang</em>, <em>Yongbin Li</em></p></li><li><p>Marathon: A Race Through the Realm of Long Context with Large Language Models, <strong>[ACL'24, CCF-A]</strong></p><p><em><strong>Lei Zhang</strong></em>, <em>Yunshui Li</em>, <em>Ziqiang Liu</em>, <em>Jiaxi Yang</em>, <em>Junhao Liu</em>, <em>Longze Chen</em>, <em>Run Luo</em>, <em>Min Yang</em></p></li><li><p>One Shot Learning as Instruction Data Prospector for Large Language Models, <strong>[ACL'24, CCF-A]</strong></p><p><em>Yunshui Li</em>, <em>Binyuan Hui</em>, <em>Xiaobo Xia</em>, <em>Jiaxi Yang</em>, <em>Min Yang</em>, <em><strong>Lei Zhang</strong></em>, <em>Shuzheng Si</em>, <em>Junhao Liu</em>, <em>Tongliang Liu</em>, <em>Fei Huang</em>, <em>Yongbin Li</em></p></li><li><p>Image-text retrieval via contrastive learning with auxiliary generative features and support-set regularization, <strong>[SIGIR'22, CCF-A]</strong></p><p><em><strong>Lei Zhang</strong></em>, <em>Min Yang</em>, <em>Chengming Li</em>, <em>Ruifeng Xu</em></p></li></ul><h2 id=research-interests>Research Interests<a hidden class=anchor aria-hidden=true href=#research-interests>#</a></h2><ul><li><strong>Agentic Coding</strong></li><li><strong>Agentic Reinforcement Learning</strong></li><li><strong>Natural Language Processing</strong></li></ul><h2 id=contact>Contact<a hidden class=anchor aria-hidden=true href=#contact>#</a></h2><p>Feel free to reach out via <a href=mailto:lei.zhang2@siat.ac.cn>lei.zhang2@siat.ac.cn</a> for collaboration or questions.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://hambaobao.me/>Homepage</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>